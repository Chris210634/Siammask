{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65794c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# SiamMask\n",
    "# Licensed under The MIT License\n",
    "# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n",
    "# --------------------------------------------------------\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from os import makedirs\n",
    "from os.path import join, isdir, isfile\n",
    "\n",
    "from utils.log_helper import init_log, add_file_handler\n",
    "from utils.load_helper import load_pretrain\n",
    "from utils.bbox_helper import get_axis_aligned_bbox, cxy_wh_2_rect\n",
    "from utils.benchmark_helper import load_dataset, dataset_zoo\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.anchors import Anchors\n",
    "from utils.tracker_config import TrackerConfig\n",
    "\n",
    "from utils.config_helper import load_config\n",
    "from utils.pyvotkit.region import vot_overlap, vot_float2str\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thrs = np.arange(0.3, 0.5, 0.05)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test SiamMask')\n",
    "parser.add_argument('--arch', dest='arch', default='', choices=['Custom',],\n",
    "                    help='architecture of pretrained model')\n",
    "parser.add_argument('--config', dest='config', required=True, help='hyper-parameter for SiamMask')\n",
    "parser.add_argument('--resume', default='', type=str, required=True,\n",
    "                    metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--mask', action='store_true', help='whether use mask output')\n",
    "parser.add_argument('--refine', action='store_true', help='whether use mask refine output')\n",
    "parser.add_argument('--dataset', dest='dataset', default='VOT2018', choices=dataset_zoo,\n",
    "                    help='datasets')\n",
    "parser.add_argument('-l', '--log', default=\"log_test.txt\", type=str, help='log file')\n",
    "parser.add_argument('-v', '--visualization', dest='visualization', action='store_true',\n",
    "                    help='whether visualize result')\n",
    "parser.add_argument('--save_mask', action='store_true', help='whether use save mask for davis')\n",
    "parser.add_argument('--gt', action='store_true', help='whether use gt rect for davis (Oracle)')\n",
    "parser.add_argument('--video', default='', type=str, help='test special video')\n",
    "parser.add_argument('--cpu', action='store_true', help='cpu mode')\n",
    "parser.add_argument('--debug', action='store_true', help='debug mode')\n",
    "\n",
    "\n",
    "def to_torch(ndarray):\n",
    "    if type(ndarray).__module__ == 'numpy':\n",
    "        return torch.from_numpy(ndarray)\n",
    "    elif not torch.is_tensor(ndarray):\n",
    "        raise ValueError(\"Cannot convert {} to torch tensor\"\n",
    "                         .format(type(ndarray)))\n",
    "    return ndarray\n",
    "\n",
    "\n",
    "def im_to_torch(img):\n",
    "    img = np.transpose(img, (2, 0, 1))  # C*H*W\n",
    "    img = to_torch(img).float()\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans, out_mode='torch'):\n",
    "    if isinstance(pos, float):\n",
    "        pos = [pos, pos]\n",
    "    sz = original_sz\n",
    "    im_sz = im.shape\n",
    "    c = (original_sz + 1) / 2\n",
    "    context_xmin = round(pos[0] - c)\n",
    "    context_xmax = context_xmin + sz - 1\n",
    "    context_ymin = round(pos[1] - c)\n",
    "    context_ymax = context_ymin + sz - 1\n",
    "    left_pad = int(max(0., -context_xmin))\n",
    "    top_pad = int(max(0., -context_ymin))\n",
    "    right_pad = int(max(0., context_xmax - im_sz[1] + 1))\n",
    "    bottom_pad = int(max(0., context_ymax - im_sz[0] + 1))\n",
    "\n",
    "    context_xmin = context_xmin + left_pad\n",
    "    context_xmax = context_xmax + left_pad\n",
    "    context_ymin = context_ymin + top_pad\n",
    "    context_ymax = context_ymax + top_pad\n",
    "\n",
    "    # zzp: a more easy speed version\n",
    "    r, c, k = im.shape\n",
    "    if any([top_pad, bottom_pad, left_pad, right_pad]):\n",
    "        te_im = np.zeros((r + top_pad + bottom_pad, c + left_pad + right_pad, k), np.uint8)\n",
    "        te_im[top_pad:top_pad + r, left_pad:left_pad + c, :] = im\n",
    "        if top_pad:\n",
    "            te_im[0:top_pad, left_pad:left_pad + c, :] = avg_chans\n",
    "        if bottom_pad:\n",
    "            te_im[r + top_pad:, left_pad:left_pad + c, :] = avg_chans\n",
    "        if left_pad:\n",
    "            te_im[:, 0:left_pad, :] = avg_chans\n",
    "        if right_pad:\n",
    "            te_im[:, c + left_pad:, :] = avg_chans\n",
    "        im_patch_original = te_im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n",
    "    else:\n",
    "        im_patch_original = im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n",
    "\n",
    "    if not np.array_equal(model_sz, original_sz):\n",
    "        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))\n",
    "    else:\n",
    "        im_patch = im_patch_original\n",
    "    # cv2.imshow('crop', im_patch)\n",
    "    # cv2.waitKey(0)\n",
    "    return im_to_torch(im_patch) if out_mode in 'torch' else im_patch\n",
    "\n",
    "\n",
    "def generate_anchor(cfg, score_size):\n",
    "    anchors = Anchors(cfg)\n",
    "    anchor = anchors.anchors\n",
    "    x1, y1, x2, y2 = anchor[:, 0], anchor[:, 1], anchor[:, 2], anchor[:, 3]\n",
    "    anchor = np.stack([(x1+x2)*0.5, (y1+y2)*0.5, x2-x1, y2-y1], 1)\n",
    "\n",
    "    total_stride = anchors.stride\n",
    "    anchor_num = anchor.shape[0]\n",
    "\n",
    "    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n",
    "    ori = - (score_size // 2) * total_stride\n",
    "    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n",
    "                         [ori + total_stride * dy for dy in range(score_size)])\n",
    "    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n",
    "             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n",
    "    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def siamese_init(im, target_pos, target_sz, model, hp=None, device='cpu'):\n",
    "    state = dict()\n",
    "    state['im_h'] = im.shape[0]\n",
    "    state['im_w'] = im.shape[1]\n",
    "    p = TrackerConfig()\n",
    "    p.update(hp, model.anchors)\n",
    "\n",
    "    p.renew()\n",
    "\n",
    "    net = model\n",
    "    p.scales = model.anchors['scales']\n",
    "    p.ratios = model.anchors['ratios']\n",
    "    p.anchor_num = model.anchor_num\n",
    "    p.anchor = generate_anchor(model.anchors, p.score_size)\n",
    "    avg_chans = np.mean(im, axis=(0, 1))\n",
    "\n",
    "    wc_z = target_sz[0] + p.context_amount * sum(target_sz)\n",
    "    hc_z = target_sz[1] + p.context_amount * sum(target_sz)\n",
    "    s_z = round(np.sqrt(wc_z * hc_z))\n",
    "    # initialize the exemplar\n",
    "    z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans)\n",
    "\n",
    "    z = Variable(z_crop.unsqueeze(0))\n",
    "    net.template(z.to(device))\n",
    "\n",
    "    if p.windowing == 'cosine':\n",
    "        window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size))\n",
    "    elif p.windowing == 'uniform':\n",
    "        window = np.ones((p.score_size, p.score_size))\n",
    "    window = np.tile(window.flatten(), p.anchor_num)\n",
    "\n",
    "    state['p'] = p\n",
    "    state['net'] = net\n",
    "    state['avg_chans'] = avg_chans\n",
    "    state['window'] = window\n",
    "    state['target_pos'] = target_pos\n",
    "    state['target_sz'] = target_sz\n",
    "    return state\n",
    "\n",
    "\n",
    "def siamese_track(state, im, i, mask_enable=False, refine_enable=False, device='cpu', debug=False):\n",
    "    p = state['p']\n",
    "    net = state['net']\n",
    "    avg_chans = state['avg_chans']\n",
    "    window = state['window']\n",
    "    target_pos = state['target_pos']\n",
    "    target_sz = state['target_sz']\n",
    "\n",
    "    wc_x = target_sz[1] + p.context_amount * sum(target_sz)\n",
    "    hc_x = target_sz[0] + p.context_amount * sum(target_sz)\n",
    "    s_x = np.sqrt(wc_x * hc_x)\n",
    "    scale_x = p.exemplar_size / s_x\n",
    "    d_search = (p.instance_size - p.exemplar_size) / 2\n",
    "    pad = d_search / scale_x\n",
    "    s_x = s_x + 2 * pad\n",
    "    crop_box = [target_pos[0] - round(s_x) / 2, target_pos[1] - round(s_x) / 2, round(s_x), round(s_x)]\n",
    "\n",
    "    if debug:\n",
    "        im_debug = im.copy()\n",
    "        crop_box_int = np.int0(crop_box)\n",
    "        cv2.rectangle(im_debug, (crop_box_int[0], crop_box_int[1]),\n",
    "                      (crop_box_int[0] + crop_box_int[2], crop_box_int[1] + crop_box_int[3]), (255, 0, 0), 2)\n",
    "        cv2.imshow('search area', im_debug)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    # extract scaled crops for search region x at previous target position\n",
    "    x_crop = Variable(get_subwindow_tracking(im, target_pos, p.instance_size, round(s_x), avg_chans).unsqueeze(0))\n",
    "\n",
    "    if mask_enable:\n",
    "        score, delta, mask = net.track_mask(x_crop.to(device))\n",
    "    else:\n",
    "        score, delta = net.track(x_crop.to(device))\n",
    "\n",
    "    delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n",
    "    score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1).permute(1, 0), dim=1).data[:,\n",
    "            1].cpu().numpy()\n",
    "\n",
    "    delta[0, :] = delta[0, :] * p.anchor[:, 2] + p.anchor[:, 0]\n",
    "    delta[1, :] = delta[1, :] * p.anchor[:, 3] + p.anchor[:, 1]\n",
    "    delta[2, :] = np.exp(delta[2, :]) * p.anchor[:, 2]\n",
    "    delta[3, :] = np.exp(delta[3, :]) * p.anchor[:, 3]\n",
    "\n",
    "    def change(r):\n",
    "        return np.maximum(r, 1. / r)\n",
    "\n",
    "    def sz(w, h):\n",
    "        pad = (w + h) * 0.5\n",
    "        sz2 = (w + pad) * (h + pad)\n",
    "        return np.sqrt(sz2)\n",
    "\n",
    "    def sz_wh(wh):\n",
    "        pad = (wh[0] + wh[1]) * 0.5\n",
    "        sz2 = (wh[0] + pad) * (wh[1] + pad)\n",
    "        return np.sqrt(sz2)\n",
    "\n",
    "    # size penalty\n",
    "    target_sz_in_crop = target_sz*scale_x\n",
    "    s_c = change(sz(delta[2, :], delta[3, :]) / (sz_wh(target_sz_in_crop)))  # scale penalty\n",
    "    r_c = change((target_sz_in_crop[0] / target_sz_in_crop[1]) / (delta[2, :] / delta[3, :]))  # ratio penalty\n",
    "\n",
    "    penalty = np.exp(-(r_c * s_c - 1) * p.penalty_k)\n",
    "    pscore = penalty * score\n",
    "\n",
    "    # cos window (motion model)\n",
    "    pscore = pscore * (1 - p.window_influence) + window * p.window_influence\n",
    "    #best_pscore_id = np.argmax(pscore)\n",
    "    \n",
    "    best_pscore_id = pscore.argsort()[-i]\n",
    "\n",
    "    pred_in_crop = delta[:, best_pscore_id] / scale_x\n",
    "    lr = penalty[best_pscore_id] * score[best_pscore_id] * p.lr  # lr for OTB\n",
    "\n",
    "    res_x = pred_in_crop[0] + target_pos[0]\n",
    "    res_y = pred_in_crop[1] + target_pos[1]\n",
    "\n",
    "    res_w = target_sz[0] * (1 - lr) + pred_in_crop[2] * lr\n",
    "    res_h = target_sz[1] * (1 - lr) + pred_in_crop[3] * lr\n",
    "\n",
    "    target_pos = np.array([res_x, res_y])\n",
    "    target_sz = np.array([res_w, res_h])\n",
    "\n",
    "    # for Mask Branch\n",
    "    if mask_enable:\n",
    "        best_pscore_id_mask = np.unravel_index(best_pscore_id, (5, p.score_size, p.score_size))\n",
    "        delta_x, delta_y = best_pscore_id_mask[2], best_pscore_id_mask[1]\n",
    "\n",
    "        if refine_enable:\n",
    "            mask = net.track_refine((delta_y, delta_x)).to(device).sigmoid().squeeze().view(\n",
    "                p.out_size, p.out_size).cpu().data.numpy()\n",
    "        else:\n",
    "            mask = mask[0, :, delta_y, delta_x].sigmoid(). \\\n",
    "                squeeze().view(p.out_size, p.out_size).cpu().data.numpy()\n",
    "\n",
    "        def crop_back(image, bbox, out_sz, padding=-1):\n",
    "            a = (out_sz[0] - 1) / bbox[2]\n",
    "            b = (out_sz[1] - 1) / bbox[3]\n",
    "            c = -a * bbox[0]\n",
    "            d = -b * bbox[1]\n",
    "            mapping = np.array([[a, 0, c],\n",
    "                                [0, b, d]]).astype(np.float)\n",
    "            crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n",
    "                                  flags=cv2.INTER_LINEAR,\n",
    "                                  borderMode=cv2.BORDER_CONSTANT,\n",
    "                                  borderValue=padding)\n",
    "            return crop\n",
    "\n",
    "        s = crop_box[2] / p.instance_size\n",
    "        sub_box = [crop_box[0] + (delta_x - p.base_size / 2) * p.total_stride * s,\n",
    "                   crop_box[1] + (delta_y - p.base_size / 2) * p.total_stride * s,\n",
    "                   s * p.exemplar_size, s * p.exemplar_size]\n",
    "        s = p.out_size / sub_box[2]\n",
    "        back_box = [-sub_box[0] * s, -sub_box[1] * s, state['im_w'] * s, state['im_h'] * s]\n",
    "        mask_in_img = crop_back(mask, back_box, (state['im_w'], state['im_h']))\n",
    "\n",
    "        target_mask = (mask_in_img > p.seg_thr).astype(np.uint8)\n",
    "        if cv2.__version__[-5] == '4':\n",
    "            contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        else:\n",
    "            _, contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n",
    "        if len(contours) != 0 and np.max(cnt_area) > 100:\n",
    "            contour = contours[np.argmax(cnt_area)]  # use max area polygon\n",
    "            polygon = contour.reshape(-1, 2)\n",
    "            # pbox = cv2.boundingRect(polygon)  # Min Max Rectangle\n",
    "            prbox = cv2.boxPoints(cv2.minAreaRect(polygon))  # Rotated Rectangle\n",
    "\n",
    "            # box_in_img = pbox\n",
    "            rbox_in_img = prbox\n",
    "        else:  # empty mask\n",
    "            location = cxy_wh_2_rect(target_pos, target_sz)\n",
    "            rbox_in_img = np.array([[location[0], location[1]],\n",
    "                                    [location[0] + location[2], location[1]],\n",
    "                                    [location[0] + location[2], location[1] + location[3]],\n",
    "                                    [location[0], location[1] + location[3]]])\n",
    "\n",
    "    target_pos[0] = max(0, min(state['im_w'], target_pos[0]))\n",
    "    target_pos[1] = max(0, min(state['im_h'], target_pos[1]))\n",
    "    target_sz[0] = max(10, min(state['im_w'], target_sz[0]))\n",
    "    target_sz[1] = max(10, min(state['im_h'], target_sz[1]))\n",
    "\n",
    "    state['target_pos'] = target_pos\n",
    "    state['target_sz'] = target_sz\n",
    "    state['score'] = score[best_pscore_id]\n",
    "    state['mask'] = mask_in_img if mask_enable else []\n",
    "    state['ploygon'] = rbox_in_img if mask_enable else []\n",
    "    return state\n",
    "\n",
    "\n",
    "def track_vot(model, video, hp=None, mask_enable=False, refine_enable=False, device='cpu'):\n",
    "    regions = []  # result and states[1 init / 2 lost / 0 skip]\n",
    "    image_files, gt = video['image_files'], video['gt']\n",
    "\n",
    "    start_frame, end_frame, lost_times, toc = 0, len(image_files), 0, 0\n",
    "\n",
    "    for f, image_file in enumerate(image_files):\n",
    "        im = cv2.imread(image_file)\n",
    "        #img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img2)\n",
    "        im_show = im.copy()\n",
    "        \n",
    "        tic = cv2.getTickCount()\n",
    "        for ii in range(1, 101,10):\n",
    "            if f == start_frame:  # init\n",
    "                cx, cy, w, h = get_axis_aligned_bbox(gt[f])\n",
    "                target_pos = np.array([cx, cy])\n",
    "                target_sz = np.array([w, h])\n",
    "                state = siamese_init(im, target_pos, target_sz, model, hp, device)  # init tracker\n",
    "                location = cxy_wh_2_rect(state['target_pos'], state['target_sz'])\n",
    "                regions.append(1 if 'VOT' in args.dataset else gt[f])\n",
    "            elif f > start_frame:  # tracking\n",
    "                state = siamese_track(state, im, ii, mask_enable, refine_enable, device, args.debug)  # track\n",
    "                if mask_enable:\n",
    "                    location = state['ploygon'].flatten()\n",
    "                    mask = state['mask']\n",
    "                else:\n",
    "                    location = cxy_wh_2_rect(state['target_pos'], state['target_sz'])\n",
    "                    mask = []\n",
    "\n",
    "                if 'VOT' in args.dataset:\n",
    "                    gt_polygon = ((gt[f][0], gt[f][1]), (gt[f][2], gt[f][3]),\n",
    "                                  (gt[f][4], gt[f][5]), (gt[f][6], gt[f][7]))\n",
    "                    if mask_enable:\n",
    "                        pred_polygon = ((location[0], location[1]), (location[2], location[3]),\n",
    "                                        (location[4], location[5]), (location[6], location[7]))\n",
    "                    else:\n",
    "                        pred_polygon = ((location[0], location[1]),\n",
    "                                        (location[0] + location[2], location[1]),\n",
    "                                        (location[0] + location[2], location[1] + location[3]),\n",
    "                                        (location[0], location[1] + location[3]))\n",
    "                    b_overlap = vot_overlap(gt_polygon, pred_polygon, (im.shape[1], im.shape[0]))\n",
    "                else:\n",
    "                    b_overlap = 1\n",
    "\n",
    "                if b_overlap:\n",
    "                    regions.append(location)\n",
    "                else:  # lost\n",
    "                    regions.append(2)\n",
    "                    lost_times += 1\n",
    "                    start_frame = f + 5  # skip 5 frames\n",
    "            else:  # skip\n",
    "                regions.append(0)\n",
    "            toc += cv2.getTickCount() - tic\n",
    "\n",
    "            #if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n",
    "            if f >= start_frame:\n",
    "\n",
    "                if f == 0: cv2.destroyAllWindows()\n",
    "                if gt.shape[0] > f:\n",
    "                    if len(gt[f]) == 8:\n",
    "                        cv2.polylines(im_show, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n",
    "                        pass\n",
    "                    else:\n",
    "                        cv2.rectangle(im_show, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n",
    "                        pass\n",
    "                if len(location) == 8:\n",
    "                    if mask_enable:\n",
    "                        mask = mask > state['p'].seg_thr\n",
    "                        #im_show[:, :, 2] = mask * 255 + (1 - mask) * im_show[:, :, 2]\n",
    "                    location_int = np.int0(location)\n",
    "                    cv2.polylines(im_show, [location_int.reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n",
    "                else:\n",
    "                    location = [int(l) for l in location]\n",
    "                    #cv2.rectangle(im_show, (location[0], location[1]),\n",
    "                     #             (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n",
    "                #cv2.rectangle(im_show, (location[0], location[1]),\n",
    "                 #               (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n",
    "        cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        cv2.putText(im_show, str(lost_times), (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(im_show, str(state['score']) if 'score' in state else '', (40, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(video['name'], im_show)\n",
    "        cv2.waitKey(1)\n",
    "            \n",
    "            \n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    # save result\n",
    "    name = args.arch.split('.')[0] + '_' + ('mask_' if mask_enable else '') + ('refine_' if refine_enable else '') +\\\n",
    "           args.resume.split('/')[-1].split('.')[0]\n",
    "\n",
    "    if 'VOT' in args.dataset:\n",
    "        video_path = join('test', args.dataset, name,\n",
    "                          'baseline', video['name'])\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        result_path = join(video_path, '{:s}_001.txt'.format(video['name']))\n",
    "        with open(result_path, \"w\") as fin:\n",
    "            for x in regions:\n",
    "                fin.write(\"{:d}\\n\".format(x)) if isinstance(x, int) else \\\n",
    "                        fin.write(','.join([vot_float2str(\"%.4f\", i) for i in x]) + '\\n')\n",
    "    else:  # OTB\n",
    "        video_path = join('test', args.dataset, name)\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        result_path = join(video_path, '{:s}.txt'.format(video['name']))\n",
    "        with open(result_path, \"w\") as fin:\n",
    "            for x in regions:\n",
    "                fin.write(','.join([str(i) for i in x])+'\\n')\n",
    "\n",
    "    logger.info('({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps Lost: {:d}'.format(\n",
    "        v_id, video['name'], toc, f / toc, lost_times))\n",
    "\n",
    "    return lost_times, f / toc\n",
    "\n",
    "\n",
    "def MultiBatchIouMeter(thrs, outputs, targets, start=None, end=None):\n",
    "    targets = np.array(targets)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_frame = targets.shape[0]\n",
    "    if start is None:\n",
    "        object_ids = np.array(list(range(outputs.shape[0]))) + 1\n",
    "    else:\n",
    "        object_ids = [int(id) for id in start]\n",
    "\n",
    "    num_object = len(object_ids)\n",
    "    res = np.zeros((num_object, len(thrs)), dtype=np.float32)\n",
    "\n",
    "    output_max_id = np.argmax(outputs, axis=0).astype('uint8')+1\n",
    "    outputs_max = np.max(outputs, axis=0)\n",
    "    for k, thr in enumerate(thrs):\n",
    "        output_thr = outputs_max > thr\n",
    "        for j in range(num_object):\n",
    "            target_j = targets == object_ids[j]\n",
    "\n",
    "            if start is None:\n",
    "                start_frame, end_frame = 1, num_frame - 1\n",
    "            else:\n",
    "                start_frame, end_frame = start[str(object_ids[j])] + 1, end[str(object_ids[j])] - 1\n",
    "            iou = []\n",
    "            for i in range(start_frame, end_frame):\n",
    "                pred = (output_thr[i] * output_max_id[i]) == (j+1)\n",
    "                mask_sum = (pred == 1).astype(np.uint8) + (target_j[i] > 0).astype(np.uint8)\n",
    "                intxn = np.sum(mask_sum == 2)\n",
    "                union = np.sum(mask_sum > 0)\n",
    "                if union > 0:\n",
    "                    iou.append(intxn / union)\n",
    "                elif union == 0 and intxn == 0:\n",
    "                    iou.append(1)\n",
    "            res[j, k] = np.mean(iou)\n",
    "    return res\n",
    "\n",
    "\n",
    "def track_vos(model, video, hp=None, mask_enable=False, refine_enable=False, mot_enable=False, device='cpu'):\n",
    "    image_files = video['image_files']\n",
    "\n",
    "    annos = [np.array(Image.open(x)) for x in video['anno_files']]\n",
    "    if 'anno_init_files' in video:\n",
    "        annos_init = [np.array(Image.open(x)) for x in video['anno_init_files']]\n",
    "    else:\n",
    "        annos_init = [annos[0]]\n",
    "\n",
    "    if not mot_enable:\n",
    "        annos = [(anno > 0).astype(np.uint8) for anno in annos]\n",
    "        annos_init = [(anno_init > 0).astype(np.uint8) for anno_init in annos_init]\n",
    "\n",
    "    if 'start_frame' in video:\n",
    "        object_ids = [int(id) for id in video['start_frame']]\n",
    "    else:\n",
    "        object_ids = [o_id for o_id in np.unique(annos[0]) if o_id != 0]\n",
    "        if len(object_ids) != len(annos_init):\n",
    "            annos_init = annos_init*len(object_ids)\n",
    "    object_num = len(object_ids)\n",
    "    toc = 0\n",
    "    pred_masks = np.zeros((object_num, len(image_files), annos[0].shape[0], annos[0].shape[1]))-1\n",
    "    for obj_id, o_id in enumerate(object_ids):\n",
    "\n",
    "        if 'start_frame' in video:\n",
    "            start_frame = video['start_frame'][str(o_id)]\n",
    "            end_frame = video['end_frame'][str(o_id)]\n",
    "        else:\n",
    "            start_frame, end_frame = 0, len(image_files)\n",
    "\n",
    "        for f, image_file in enumerate(image_files):\n",
    "            im = cv2.imread(image_file)\n",
    "            tic = cv2.getTickCount()\n",
    "            if f == start_frame:  # init\n",
    "                mask = annos_init[obj_id] == o_id\n",
    "                x, y, w, h = cv2.boundingRect((mask).astype(np.uint8))\n",
    "                cx, cy = x + w/2, y + h/2\n",
    "                target_pos = np.array([cx, cy])\n",
    "                target_sz = np.array([w, h])\n",
    "                state = siamese_init(im, target_pos, target_sz, model, hp, device=device)  # init tracker\n",
    "            elif end_frame >= f > start_frame:  # tracking\n",
    "                state = siamese_track(state, im, mask_enable, refine_enable, device=device)  # track\n",
    "                mask = state['mask']\n",
    "            toc += cv2.getTickCount() - tic\n",
    "            if end_frame >= f >= start_frame:\n",
    "                pred_masks[obj_id, f, :, :] = mask\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    if len(annos) == len(image_files):\n",
    "        multi_mean_iou = MultiBatchIouMeter(thrs, pred_masks, annos,\n",
    "                                            start=video['start_frame'] if 'start_frame' in video else None,\n",
    "                                            end=video['end_frame'] if 'end_frame' in video else None)\n",
    "        for i in range(object_num):\n",
    "            for j, thr in enumerate(thrs):\n",
    "                logger.info('Fusion Multi Object{:20s} IOU at {:.2f}: {:.4f}'.format(video['name'] + '_' + str(i + 1), thr,\n",
    "                                                                           multi_mean_iou[i, j]))\n",
    "    else:\n",
    "        multi_mean_iou = []\n",
    "\n",
    "    if args.save_mask:\n",
    "        video_path = join('test', args.dataset, 'SiamMask', video['name'])\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        pred_mask_final = np.array(pred_masks)\n",
    "        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype('uint8') + 1) * (\n",
    "                np.max(pred_mask_final, axis=0) > state['p'].seg_thr).astype('uint8')\n",
    "        for i in range(pred_mask_final.shape[0]):\n",
    "            cv2.imwrite(join(video_path, image_files[i].split('/')[-1].split('.')[0] + '.png'), pred_mask_final[i].astype(np.uint8))\n",
    "\n",
    "    if args.visualization:\n",
    "        pred_mask_final = np.array(pred_masks)\n",
    "        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype('uint8') + 1) * (\n",
    "                np.max(pred_mask_final, axis=0) > state['p'].seg_thr).astype('uint8')\n",
    "        COLORS = np.random.randint(128, 255, size=(object_num, 3), dtype=\"uint8\")\n",
    "        COLORS = np.vstack([[0, 0, 0], COLORS]).astype(\"uint8\")\n",
    "        mask = COLORS[pred_mask_final]\n",
    "        for f, image_file in enumerate(image_files):\n",
    "            output = ((0.4 * cv2.imread(image_file)) + (0.6 * mask[f,:,:,:])).astype(\"uint8\")\n",
    "            cv2.imshow(\"mask\", output)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    logger.info('({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps'.format(\n",
    "        v_id, video['name'], toc, f*len(object_ids) / toc))\n",
    "\n",
    "    return multi_mean_iou, f*len(object_ids) / toc\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, logger, v_id\n",
    "    args = parser.parse_args()\n",
    "    cfg = load_config(args)\n",
    "\n",
    "    init_log('global', logging.INFO)\n",
    "    if args.log != \"\":\n",
    "        add_file_handler('global', args.log, logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger('global')\n",
    "    logger.info(args)\n",
    "\n",
    "    # setup model\n",
    "    if args.arch == 'Custom':\n",
    "        from custom import Custom\n",
    "        model = Custom(anchors=cfg['anchors'])\n",
    "    else:\n",
    "        parser.error('invalid architecture: {}'.format(args.arch))\n",
    "\n",
    "    if args.resume:\n",
    "        assert isfile(args.resume), '{} is not a valid file'.format(args.resume)\n",
    "        model = load_pretrain(model, args.resume)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if (torch.cuda.is_available() and not args.cpu) else 'cpu')\n",
    "    model = model.to(device)\n",
    "    # setup dataset\n",
    "    dataset = load_dataset(args.dataset)\n",
    "\n",
    "    # VOS or VOT?\n",
    "    if args.dataset in ['DAVIS2016', 'DAVIS2017', 'ytb_vos'] and args.mask:\n",
    "        vos_enable = True  # enable Mask output\n",
    "    else:\n",
    "        vos_enable = False\n",
    "\n",
    "    total_lost = 0  # VOT\n",
    "    iou_lists = []  # VOS\n",
    "    speed_list = []\n",
    "\n",
    "    for v_id, video in enumerate(dataset.keys(), start=1):\n",
    "        if args.video != '' and video != args.video:\n",
    "            continue\n",
    "\n",
    "        if vos_enable:\n",
    "            iou_list, speed = track_vos(model, dataset[video], cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                                 args.mask, args.refine, args.dataset in ['DAVIS2017', 'ytb_vos'], device=device)\n",
    "            iou_lists.append(iou_list)\n",
    "        else:\n",
    "            lost, speed = track_vot(model, dataset[video], cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                             args.mask, args.refine, device=device)\n",
    "            total_lost += lost\n",
    "        speed_list.append(speed)\n",
    "\n",
    "    # report final result\n",
    "    if vos_enable:\n",
    "        for thr, iou in zip(thrs, np.mean(np.concatenate(iou_lists), axis=0)):\n",
    "            logger.info('Segmentation Threshold {:.2f} mIoU: {:.3f}'.format(thr, iou))\n",
    "    else:\n",
    "        logger.info('Total Lost: {:d}'.format(total_lost))\n",
    "\n",
    "    logger.info('Mean Speed: {:.2f} FPS'.format(np.mean(speed_list)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bf96649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# SiamMask\n",
    "# Licensed under The MIT License\n",
    "# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n",
    "# --------------------------------------------------------\n",
    "from __future__ import division\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from os import makedirs\n",
    "from os.path import join, isdir, isfile\n",
    "\n",
    "from utils.log_helper import init_log, add_file_handler\n",
    "from utils.load_helper import load_pretrain\n",
    "from utils.bbox_helper import get_axis_aligned_bbox, cxy_wh_2_rect\n",
    "from utils.benchmark_helper import load_dataset, dataset_zoo\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.anchors import Anchors\n",
    "from utils.tracker_config import TrackerConfig\n",
    "\n",
    "from utils.config_helper import load_config\n",
    "from utils.pyvotkit.region import vot_overlap, vot_float2str\n",
    "\n",
    "thrs = np.arange(0.3, 0.5, 0.05)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test SiamMask')\n",
    "parser.add_argument('--arch', dest='arch', default='', choices=['Custom',],\n",
    "                    help='architecture of pretrained model')\n",
    "parser.add_argument('--config', dest='config', required=True, help='hyper-parameter for SiamMask')\n",
    "parser.add_argument('--resume', default='', type=str, required=True,\n",
    "                    metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--mask', action='store_true', help='whether use mask output')\n",
    "parser.add_argument('--refine', action='store_true', help='whether use mask refine output')\n",
    "parser.add_argument('--dataset', dest='dataset', default='VOT2018', choices=dataset_zoo,\n",
    "                    help='datasets')\n",
    "parser.add_argument('-l', '--log', default=\"log_test.txt\", type=str, help='log file')\n",
    "parser.add_argument('-v', '--visualization', dest='visualization', action='store_true',\n",
    "                    help='whether visualize result')\n",
    "parser.add_argument('--save_mask', action='store_true', help='whether use save mask for davis')\n",
    "parser.add_argument('--gt', action='store_true', help='whether use gt rect for davis (Oracle)')\n",
    "parser.add_argument('--video', default='', type=str, help='test special video')\n",
    "parser.add_argument('--cpu', action='store_true', help='cpu mode')\n",
    "parser.add_argument('--debug', action='store_true', help='debug mode')\n",
    "\n",
    "\n",
    "def to_torch(ndarray):\n",
    "    if type(ndarray).__module__ == 'numpy':\n",
    "        return torch.from_numpy(ndarray)\n",
    "    elif not torch.is_tensor(ndarray):\n",
    "        raise ValueError(\"Cannot convert {} to torch tensor\"\n",
    "                         .format(type(ndarray)))\n",
    "    return ndarray\n",
    "\n",
    "\n",
    "def im_to_torch(img):\n",
    "    img = np.transpose(img, (2, 0, 1))  # C*H*W\n",
    "    img = to_torch(img).float()\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans, out_mode='torch'):\n",
    "    if isinstance(pos, float):\n",
    "        pos = [pos, pos]\n",
    "    sz = original_sz\n",
    "    im_sz = im.shape\n",
    "    c = (original_sz + 1) / 2\n",
    "    context_xmin = round(pos[0] - c)\n",
    "    context_xmax = context_xmin + sz - 1\n",
    "    context_ymin = round(pos[1] - c)\n",
    "    context_ymax = context_ymin + sz - 1\n",
    "    left_pad = int(max(0., -context_xmin))\n",
    "    top_pad = int(max(0., -context_ymin))\n",
    "    right_pad = int(max(0., context_xmax - im_sz[1] + 1))\n",
    "    bottom_pad = int(max(0., context_ymax - im_sz[0] + 1))\n",
    "\n",
    "    context_xmin = context_xmin + left_pad\n",
    "    context_xmax = context_xmax + left_pad\n",
    "    context_ymin = context_ymin + top_pad\n",
    "    context_ymax = context_ymax + top_pad\n",
    "\n",
    "    # zzp: a more easy speed version\n",
    "    r, c, k = im.shape\n",
    "    if any([top_pad, bottom_pad, left_pad, right_pad]):\n",
    "        te_im = np.zeros((r + top_pad + bottom_pad, c + left_pad + right_pad, k), np.uint8)\n",
    "        te_im[top_pad:top_pad + r, left_pad:left_pad + c, :] = im\n",
    "        if top_pad:\n",
    "            te_im[0:top_pad, left_pad:left_pad + c, :] = avg_chans\n",
    "        if bottom_pad:\n",
    "            te_im[r + top_pad:, left_pad:left_pad + c, :] = avg_chans\n",
    "        if left_pad:\n",
    "            te_im[:, 0:left_pad, :] = avg_chans\n",
    "        if right_pad:\n",
    "            te_im[:, c + left_pad:, :] = avg_chans\n",
    "        im_patch_original = te_im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n",
    "    else:\n",
    "        im_patch_original = im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n",
    "\n",
    "    if not np.array_equal(model_sz, original_sz):\n",
    "        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))\n",
    "    else:\n",
    "        im_patch = im_patch_original\n",
    "    # cv2.imshow('crop', im_patch)\n",
    "    # cv2.waitKey(0)\n",
    "    return im_to_torch(im_patch) if out_mode in 'torch' else im_patch\n",
    "\n",
    "\n",
    "def generate_anchor(cfg, score_size):\n",
    "    anchors = Anchors(cfg)\n",
    "    anchor = anchors.anchors\n",
    "    x1, y1, x2, y2 = anchor[:, 0], anchor[:, 1], anchor[:, 2], anchor[:, 3]\n",
    "    anchor = np.stack([(x1+x2)*0.5, (y1+y2)*0.5, x2-x1, y2-y1], 1)\n",
    "\n",
    "    total_stride = anchors.stride\n",
    "    anchor_num = anchor.shape[0]\n",
    "\n",
    "    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n",
    "    ori = - (score_size // 2) * total_stride\n",
    "    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n",
    "                         [ori + total_stride * dy for dy in range(score_size)])\n",
    "    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n",
    "             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n",
    "    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def siamese_init(im, target_pos, target_sz, model, hp=None, device='cpu'):\n",
    "    state = dict()\n",
    "    state['im_h'] = im.shape[0]\n",
    "    state['im_w'] = im.shape[1]\n",
    "    p = TrackerConfig()\n",
    "    p.update(hp, model.anchors)\n",
    "\n",
    "    p.renew()\n",
    "\n",
    "    net = model\n",
    "    p.scales = model.anchors['scales']\n",
    "    p.ratios = model.anchors['ratios']\n",
    "    p.anchor_num = model.anchor_num\n",
    "    p.anchor = generate_anchor(model.anchors, p.score_size)\n",
    "    avg_chans = np.mean(im, axis=(0, 1))\n",
    "\n",
    "    wc_z = target_sz[0] + p.context_amount * sum(target_sz)\n",
    "    hc_z = target_sz[1] + p.context_amount * sum(target_sz)\n",
    "    s_z = round(np.sqrt(wc_z * hc_z))\n",
    "    # initialize the exemplar\n",
    "    z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans)\n",
    "\n",
    "    z = Variable(z_crop.unsqueeze(0))\n",
    "    net.template(z.to(device))\n",
    "\n",
    "    if p.windowing == 'cosine':\n",
    "        window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size))\n",
    "    elif p.windowing == 'uniform':\n",
    "        window = np.ones((p.score_size, p.score_size))\n",
    "    window = np.tile(window.flatten(), p.anchor_num)\n",
    "\n",
    "    state['p'] = p\n",
    "    state['net'] = net\n",
    "    state['avg_chans'] = avg_chans\n",
    "    state['window'] = window\n",
    "    state['target_pos'] = target_pos\n",
    "    state['target_sz'] = target_sz\n",
    "    return state\n",
    "\n",
    "\n",
    "def siamese_track(state, im, mask_enable=False, refine_enable=False, device='cpu', debug=False):\n",
    "    p = state['p']\n",
    "    net = state['net']\n",
    "    avg_chans = state['avg_chans']\n",
    "    window = state['window']\n",
    "    target_pos = state['target_pos']\n",
    "    target_sz = state['target_sz']\n",
    "\n",
    "    wc_x = target_sz[1] + p.context_amount * sum(target_sz)\n",
    "    hc_x = target_sz[0] + p.context_amount * sum(target_sz)\n",
    "    s_x = np.sqrt(wc_x * hc_x)\n",
    "    scale_x = p.exemplar_size / s_x\n",
    "    d_search = (p.instance_size - p.exemplar_size) / 2\n",
    "    pad = d_search / scale_x\n",
    "    s_x = s_x + 2 * pad\n",
    "    crop_box = [target_pos[0] - round(s_x) / 2, target_pos[1] - round(s_x) / 2, round(s_x), round(s_x)]\n",
    "\n",
    "    if debug:\n",
    "        im_debug = im.copy()\n",
    "        crop_box_int = np.int0(crop_box)\n",
    "        cv2.rectangle(im_debug, (crop_box_int[0], crop_box_int[1]),\n",
    "                      (crop_box_int[0] + crop_box_int[2], crop_box_int[1] + crop_box_int[3]), (255, 0, 0), 2)\n",
    "        cv2.imshow('search area', im_debug)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    # extract scaled crops for search region x at previous target position\n",
    "    x_crop = Variable(get_subwindow_tracking(im, target_pos, p.instance_size, round(s_x), avg_chans).unsqueeze(0))\n",
    "\n",
    "    if mask_enable:\n",
    "        score, delta, mask = net.track_mask(x_crop.to(device))\n",
    "    else:\n",
    "        score, delta = net.track(x_crop.to(device))\n",
    "\n",
    "    delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n",
    "    score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1).permute(1, 0), dim=1).data[:,\n",
    "            1].cpu().numpy()\n",
    "\n",
    "    delta[0, :] = delta[0, :] * p.anchor[:, 2] + p.anchor[:, 0]\n",
    "    delta[1, :] = delta[1, :] * p.anchor[:, 3] + p.anchor[:, 1]\n",
    "    delta[2, :] = np.exp(delta[2, :]) * p.anchor[:, 2]\n",
    "    delta[3, :] = np.exp(delta[3, :]) * p.anchor[:, 3]\n",
    "\n",
    "    def change(r):\n",
    "        return np.maximum(r, 1. / r)\n",
    "\n",
    "    def sz(w, h):\n",
    "        pad = (w + h) * 0.5\n",
    "        sz2 = (w + pad) * (h + pad)\n",
    "        return np.sqrt(sz2)\n",
    "\n",
    "    def sz_wh(wh):\n",
    "        pad = (wh[0] + wh[1]) * 0.5\n",
    "        sz2 = (wh[0] + pad) * (wh[1] + pad)\n",
    "        return np.sqrt(sz2)\n",
    "\n",
    "    # size penalty\n",
    "    target_sz_in_crop = target_sz*scale_x\n",
    "    s_c = change(sz(delta[2, :], delta[3, :]) / (sz_wh(target_sz_in_crop)))  # scale penalty\n",
    "    r_c = change((target_sz_in_crop[0] / target_sz_in_crop[1]) / (delta[2, :] / delta[3, :]))  # ratio penalty\n",
    "\n",
    "    penalty = np.exp(-(r_c * s_c - 1) * p.penalty_k)\n",
    "    pscore = penalty * score\n",
    "\n",
    "    # cos window (motion model)\n",
    "    pscore = pscore * (1 - p.window_influence) + window * p.window_influence\n",
    "    best_pscore_id = np.argmax(pscore)\n",
    "\n",
    "    pred_in_crop = delta[:, best_pscore_id] / scale_x\n",
    "    lr = penalty[best_pscore_id] * score[best_pscore_id] * p.lr  # lr for OTB\n",
    "\n",
    "    res_x = pred_in_crop[0] + target_pos[0]\n",
    "    res_y = pred_in_crop[1] + target_pos[1]\n",
    "\n",
    "    res_w = target_sz[0] * (1 - lr) + pred_in_crop[2] * lr\n",
    "    res_h = target_sz[1] * (1 - lr) + pred_in_crop[3] * lr\n",
    "\n",
    "    target_pos = np.array([res_x, res_y])\n",
    "    target_sz = np.array([res_w, res_h])\n",
    "\n",
    "    # for Mask Branch\n",
    "    if mask_enable:\n",
    "        best_pscore_id_mask = np.unravel_index(best_pscore_id, (5, p.score_size, p.score_size))\n",
    "        delta_x, delta_y = best_pscore_id_mask[2], best_pscore_id_mask[1]\n",
    "\n",
    "        if refine_enable:\n",
    "            mask = net.track_refine((delta_y, delta_x)).to(device).sigmoid().squeeze().view(\n",
    "                p.out_size, p.out_size).cpu().data.numpy()\n",
    "        else:\n",
    "            mask = mask[0, :, delta_y, delta_x].sigmoid(). \\\n",
    "                squeeze().view(p.out_size, p.out_size).cpu().data.numpy()\n",
    "\n",
    "        def crop_back(image, bbox, out_sz, padding=-1):\n",
    "            a = (out_sz[0] - 1) / bbox[2]\n",
    "            b = (out_sz[1] - 1) / bbox[3]\n",
    "            c = -a * bbox[0]\n",
    "            d = -b * bbox[1]\n",
    "            mapping = np.array([[a, 0, c],\n",
    "                                [0, b, d]]).astype(np.float)\n",
    "            crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n",
    "                                  flags=cv2.INTER_LINEAR,\n",
    "                                  borderMode=cv2.BORDER_CONSTANT,\n",
    "                                  borderValue=padding)\n",
    "            return crop\n",
    "\n",
    "        s = crop_box[2] / p.instance_size\n",
    "        sub_box = [crop_box[0] + (delta_x - p.base_size / 2) * p.total_stride * s,\n",
    "                   crop_box[1] + (delta_y - p.base_size / 2) * p.total_stride * s,\n",
    "                   s * p.exemplar_size, s * p.exemplar_size]\n",
    "        s = p.out_size / sub_box[2]\n",
    "        back_box = [-sub_box[0] * s, -sub_box[1] * s, state['im_w'] * s, state['im_h'] * s]\n",
    "        mask_in_img = crop_back(mask, back_box, (state['im_w'], state['im_h']))\n",
    "\n",
    "        target_mask = (mask_in_img > p.seg_thr).astype(np.uint8)\n",
    "        if cv2.__version__[-5] == '4':\n",
    "            contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        else:\n",
    "            _, contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n",
    "        if len(contours) != 0 and np.max(cnt_area) > 100:\n",
    "            contour = contours[np.argmax(cnt_area)]  # use max area polygon\n",
    "            polygon = contour.reshape(-1, 2)\n",
    "            # pbox = cv2.boundingRect(polygon)  # Min Max Rectangle\n",
    "            prbox = cv2.boxPoints(cv2.minAreaRect(polygon))  # Rotated Rectangle\n",
    "\n",
    "            # box_in_img = pbox\n",
    "            rbox_in_img = prbox\n",
    "        else:  # empty mask\n",
    "            location = cxy_wh_2_rect(target_pos, target_sz)\n",
    "            rbox_in_img = np.array([[location[0], location[1]],\n",
    "                                    [location[0] + location[2], location[1]],\n",
    "                                    [location[0] + location[2], location[1] + location[3]],\n",
    "                                    [location[0], location[1] + location[3]]])\n",
    "\n",
    "    target_pos[0] = max(0, min(state['im_w'], target_pos[0]))\n",
    "    target_pos[1] = max(0, min(state['im_h'], target_pos[1]))\n",
    "    target_sz[0] = max(10, min(state['im_w'], target_sz[0]))\n",
    "    target_sz[1] = max(10, min(state['im_h'], target_sz[1]))\n",
    "\n",
    "    state['target_pos'] = target_pos\n",
    "    state['target_sz'] = target_sz\n",
    "    state['score'] = score[best_pscore_id]\n",
    "    state['mask'] = mask_in_img if mask_enable else []\n",
    "    state['ploygon'] = rbox_in_img if mask_enable else []\n",
    "    return state\n",
    "\n",
    "\n",
    "def track_vot(model, video, hp=None, mask_enable=False, refine_enable=False, device='cpu'):\n",
    "    regions = []  # result and states[1 init / 2 lost / 0 skip]\n",
    "    image_files, gt = video['image_files'], video['gt']\n",
    "\n",
    "    start_frame, end_frame, lost_times, toc = 0, len(image_files), 0, 0\n",
    "\n",
    "    for f, image_file in enumerate(image_files):\n",
    "        im = cv2.imread(image_file)\n",
    "        tic = cv2.getTickCount()\n",
    "        if f == start_frame:  # init\n",
    "            cx, cy, w, h = get_axis_aligned_bbox(gt[f])\n",
    "            target_pos = np.array([cx, cy])\n",
    "            target_sz = np.array([w, h])\n",
    "            state = siamese_init(im, target_pos, target_sz, model, hp, device)  # init tracker\n",
    "            location = cxy_wh_2_rect(state['target_pos'], state['target_sz'])\n",
    "            regions.append(1 if 'VOT' in args.dataset else gt[f])\n",
    "        elif f > start_frame:  # tracking\n",
    "            state = siamese_track(state, im, mask_enable, refine_enable, device, args.debug)  # track\n",
    "            if mask_enable:\n",
    "                location = state['ploygon'].flatten()\n",
    "                mask = state['mask']\n",
    "            else:\n",
    "                location = cxy_wh_2_rect(state['target_pos'], state['target_sz'])\n",
    "                mask = []\n",
    "\n",
    "            if 'VOT' in args.dataset:\n",
    "                gt_polygon = ((gt[f][0], gt[f][1]), (gt[f][2], gt[f][3]),\n",
    "                              (gt[f][4], gt[f][5]), (gt[f][6], gt[f][7]))\n",
    "                if mask_enable:\n",
    "                    pred_polygon = ((location[0], location[1]), (location[2], location[3]),\n",
    "                                    (location[4], location[5]), (location[6], location[7]))\n",
    "                else:\n",
    "                    pred_polygon = ((location[0], location[1]),\n",
    "                                    (location[0] + location[2], location[1]),\n",
    "                                    (location[0] + location[2], location[1] + location[3]),\n",
    "                                    (location[0], location[1] + location[3]))\n",
    "                b_overlap = vot_overlap(gt_polygon, pred_polygon, (im.shape[1], im.shape[0]))\n",
    "            else:\n",
    "                b_overlap = 1\n",
    "\n",
    "            if b_overlap:\n",
    "                regions.append(location)\n",
    "            else:  # lost\n",
    "                regions.append(2)\n",
    "                lost_times += 1\n",
    "                start_frame = f + 5  # skip 5 frames\n",
    "        else:  # skip\n",
    "            regions.append(0)\n",
    "        toc += cv2.getTickCount() - tic\n",
    "\n",
    "        if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n",
    "            im_show = im.copy()\n",
    "            if f == 0: cv2.destroyAllWindows()\n",
    "            if gt.shape[0] > f:\n",
    "                if len(gt[f]) == 8:\n",
    "                    cv2.polylines(im_show, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n",
    "                else:\n",
    "                    cv2.rectangle(im_show, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n",
    "            if len(location) == 8:\n",
    "                if mask_enable:\n",
    "                    mask = mask > state['p'].seg_thr\n",
    "                    im_show[:, :, 2] = mask * 255 + (1 - mask) * im_show[:, :, 2]\n",
    "                location_int = np.int0(location)\n",
    "                cv2.polylines(im_show, [location_int.reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n",
    "            else:\n",
    "                location = [int(l) for l in location]\n",
    "                cv2.rectangle(im_show, (location[0], location[1]),\n",
    "                              (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n",
    "            cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.putText(im_show, str(lost_times), (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(im_show, str(state['score']) if 'score' in state else '', (40, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow(video['name'], im_show)\n",
    "            cv2.waitKey(1)\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    # save result\n",
    "    name = args.arch.split('.')[0] + '_' + ('mask_' if mask_enable else '') + ('refine_' if refine_enable else '') +\\\n",
    "           args.resume.split('/')[-1].split('.')[0]\n",
    "\n",
    "    if 'VOT' in args.dataset:\n",
    "        video_path = join('test', args.dataset, name,\n",
    "                          'baseline', video['name'])\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        result_path = join(video_path, '{:s}_001.txt'.format(video['name']))\n",
    "        with open(result_path, \"w\") as fin:\n",
    "            for x in regions:\n",
    "                fin.write(\"{:d}\\n\".format(x)) if isinstance(x, int) else \\\n",
    "                        fin.write(','.join([vot_float2str(\"%.4f\", i) for i in x]) + '\\n')\n",
    "    else:  # OTB\n",
    "        video_path = join('test', args.dataset, name)\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        result_path = join(video_path, '{:s}.txt'.format(video['name']))\n",
    "        with open(result_path, \"w\") as fin:\n",
    "            for x in regions:\n",
    "                fin.write(','.join([str(i) for i in x])+'\\n')\n",
    "\n",
    "    logger.info('({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps Lost: {:d}'.format(\n",
    "        v_id, video['name'], toc, f / toc, lost_times))\n",
    "\n",
    "    return lost_times, f / toc\n",
    "\n",
    "\n",
    "def MultiBatchIouMeter(thrs, outputs, targets, start=None, end=None):\n",
    "    targets = np.array(targets)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_frame = targets.shape[0]\n",
    "    if start is None:\n",
    "        object_ids = np.array(list(range(outputs.shape[0]))) + 1\n",
    "    else:\n",
    "        object_ids = [int(id) for id in start]\n",
    "\n",
    "    num_object = len(object_ids)\n",
    "    res = np.zeros((num_object, len(thrs)), dtype=np.float32)\n",
    "\n",
    "    output_max_id = np.argmax(outputs, axis=0).astype('uint8')+1\n",
    "    outputs_max = np.max(outputs, axis=0)\n",
    "    for k, thr in enumerate(thrs):\n",
    "        output_thr = outputs_max > thr\n",
    "        for j in range(num_object):\n",
    "            target_j = targets == object_ids[j]\n",
    "\n",
    "            if start is None:\n",
    "                start_frame, end_frame = 1, num_frame - 1\n",
    "            else:\n",
    "                start_frame, end_frame = start[str(object_ids[j])] + 1, end[str(object_ids[j])] - 1\n",
    "            iou = []\n",
    "            for i in range(start_frame, end_frame):\n",
    "                pred = (output_thr[i] * output_max_id[i]) == (j+1)\n",
    "                mask_sum = (pred == 1).astype(np.uint8) + (target_j[i] > 0).astype(np.uint8)\n",
    "                intxn = np.sum(mask_sum == 2)\n",
    "                union = np.sum(mask_sum > 0)\n",
    "                if union > 0:\n",
    "                    iou.append(intxn / union)\n",
    "                elif union == 0 and intxn == 0:\n",
    "                    iou.append(1)\n",
    "            res[j, k] = np.mean(iou)\n",
    "    return res\n",
    "\n",
    "\n",
    "def track_vos(model, video, video_str, hp=None, mask_enable=False, refine_enable=False, mot_enable=False, device='cpu'):\n",
    "    image_files = video['image_files']\n",
    "\n",
    "    annos = [np.array(Image.open(x)) for x in video['anno_files']]\n",
    "    if 'anno_init_files' in video:\n",
    "        annos_init = [np.array(Image.open(x)) for x in video['anno_init_files']]\n",
    "    else:\n",
    "        annos_init = [annos[0]]\n",
    "\n",
    "    if not mot_enable:\n",
    "        annos = [(anno > 0).astype(np.uint8) for anno in annos]\n",
    "        annos_init = [(anno_init > 0).astype(np.uint8) for anno_init in annos_init]\n",
    "\n",
    "    if 'start_frame' in video:\n",
    "        object_ids = [int(id) for id in video['start_frame']]\n",
    "    else:\n",
    "        object_ids = [o_id for o_id in np.unique(annos[0]) if o_id != 0]\n",
    "        if len(object_ids) != len(annos_init):\n",
    "            annos_init = annos_init*len(object_ids)\n",
    "    object_num = len(object_ids)\n",
    "    toc = 0\n",
    "    pred_masks = np.zeros((object_num, len(image_files), annos[0].shape[0], annos[0].shape[1]))-1\n",
    "    \n",
    "    images_out = []\n",
    "    \n",
    "    for obj_id, o_id in enumerate(object_ids):\n",
    "\n",
    "        if 'start_frame' in video:\n",
    "            start_frame = video['start_frame'][str(o_id)]\n",
    "            end_frame = video['end_frame'][str(o_id)]\n",
    "        else:\n",
    "            start_frame, end_frame = 0, len(image_files)\n",
    "\n",
    "        for f, image_file in enumerate(image_files):\n",
    "            im = cv2.imread(image_file)\n",
    "            tic = cv2.getTickCount()\n",
    "            if f == start_frame:  # init\n",
    "                mask = annos_init[obj_id] == o_id\n",
    "                x, y, w, h = cv2.boundingRect((mask).astype(np.uint8))\n",
    "                cx, cy = x + w/2, y + h/2\n",
    "                target_pos = np.array([cx, cy])\n",
    "                target_sz = np.array([w, h])\n",
    "                state = siamese_init(im, target_pos, target_sz, model, hp, device=device)  # init tracker\n",
    "            elif end_frame >= f > start_frame:  # tracking\n",
    "                state = siamese_track(state, im, mask_enable, refine_enable, device=device)  # track\n",
    "                mask = state['mask']\n",
    "            toc += cv2.getTickCount() - tic\n",
    "            if end_frame >= f >= start_frame:\n",
    "                pred_masks[obj_id, f, :, :] = mask\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    if len(annos) == len(image_files):\n",
    "        multi_mean_iou = MultiBatchIouMeter(thrs, pred_masks, annos,\n",
    "                                            start=video['start_frame'] if 'start_frame' in video else None,\n",
    "                                            end=video['end_frame'] if 'end_frame' in video else None)\n",
    "        for i in range(object_num):\n",
    "            for j, thr in enumerate(thrs):\n",
    "                logger.info('Fusion Multi Object{:20s} IOU at {:.2f}: {:.4f}'.format(video['name'] + '_' + str(i + 1), thr,\n",
    "                                                                           multi_mean_iou[i, j]))\n",
    "    else:\n",
    "        multi_mean_iou = []\n",
    "\n",
    "    if args.save_mask:\n",
    "        video_path = join('test', args.dataset, 'SiamMask', video['name'])\n",
    "        if not isdir(video_path): makedirs(video_path)\n",
    "        pred_mask_final = np.array(pred_masks)\n",
    "        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype('uint8') + 1) * (\n",
    "                np.max(pred_mask_final, axis=0) > state['p'].seg_thr).astype('uint8')\n",
    "        for i in range(pred_mask_final.shape[0]):\n",
    "            cv2.imwrite(join(video_path, image_files[i].split('/')[-1].split('.')[0] + '.png'), pred_mask_final[i].astype(np.uint8))\n",
    "\n",
    "    if True:\n",
    "        pred_mask_final = np.array(pred_masks)\n",
    "        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype('uint8') + 1) * (\n",
    "                np.max(pred_mask_final, axis=0) > state['p'].seg_thr).astype('uint8')\n",
    "        COLORS = np.random.randint(128, 255, size=(object_num, 3), dtype=\"uint8\")\n",
    "        COLORS = np.vstack([[0, 0, 0], COLORS]).astype(\"uint8\")\n",
    "        mask = COLORS[pred_mask_final]\n",
    "        for f, image_file in enumerate(image_files):\n",
    "            output = ((0.4 * cv2.imread(image_file)) + (0.6 * mask[f,:,:,:])).astype(\"uint8\")\n",
    "            cv2.imshow(\"mask\", output)\n",
    "            \n",
    "            if f % 10 == 0:\n",
    "                images_out.append(output)\n",
    "            #cv2.imwrite(\"/usr4/alg504/cliao25/siammask/experiments/siammask_sharp/demo_out/out{}.jpg\".format(f),output)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    vis = np.concatenate(images_out, axis=1)\n",
    "    cv2.imwrite(\"/usr4/alg504/cliao25/siammask/experiments/siammask_sharp/demo_out/{}.jpg\".format(video_str),vis)\n",
    "    \n",
    "    logger.info('({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps'.format(\n",
    "        v_id, video['name'], toc, f*len(object_ids) / toc))\n",
    "\n",
    "    return multi_mean_iou, f*len(object_ids) / toc\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, logger, v_id\n",
    "    args = parser.parse_args()\n",
    "    cfg = load_config(args)\n",
    "\n",
    "    init_log('global', logging.INFO)\n",
    "    if args.log != \"\":\n",
    "        add_file_handler('global', args.log, logging.INFO)\n",
    "\n",
    "    logger = logging.getLogger('global')\n",
    "    logger.info(args)\n",
    "\n",
    "    # setup model\n",
    "    if args.arch == 'Custom':\n",
    "        from custom import Custom\n",
    "        model = Custom(anchors=cfg['anchors'])\n",
    "    else:\n",
    "        parser.error('invalid architecture: {}'.format(args.arch))\n",
    "\n",
    "    if args.resume:\n",
    "        assert isfile(args.resume), '{} is not a valid file'.format(args.resume)\n",
    "        model = load_pretrain(model, args.resume)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if (torch.cuda.is_available() and not args.cpu) else 'cpu')\n",
    "    model = model.to(device)\n",
    "    # setup dataset\n",
    "    dataset = load_dataset(args.dataset)\n",
    "\n",
    "    # VOS or VOT?\n",
    "    if args.dataset in ['DAVIS','DAVIS2016', 'DAVIS2017', 'ytb_vos'] and args.mask:\n",
    "        vos_enable = True  # enable Mask output\n",
    "    else:\n",
    "        vos_enable = False\n",
    "\n",
    "    total_lost = 0  # VOT\n",
    "    iou_lists = []  # VOS\n",
    "    speed_list = []\n",
    "\n",
    "    for v_id, video in enumerate(dataset.keys(), start=1):\n",
    "        if args.video != '' and video != args.video:\n",
    "            continue\n",
    "\n",
    "        if vos_enable:\n",
    "            iou_list, speed = track_vos(model, dataset[video], cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                                 args.mask, args.refine, args.dataset in ['DAVIS2017', 'ytb_vos'], device=device)\n",
    "            iou_lists.append(iou_list)\n",
    "        else:\n",
    "            lost, speed = track_vot(model, dataset[video], cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                             args.mask, args.refine, device=device)\n",
    "            total_lost += lost\n",
    "        speed_list.append(speed)\n",
    "\n",
    "    # report final result\n",
    "    if vos_enable:\n",
    "        for thr, iou in zip(thrs, np.mean(np.concatenate(iou_lists), axis=0)):\n",
    "            logger.info('Segmentation Threshold {:.2f} mIoU: {:.3f}'.format(thr, iou))\n",
    "    else:\n",
    "        logger.info('Total Lost: {:d}'.format(total_lost))\n",
    "\n",
    "    logger.info('Mean Speed: {:.2f} FPS'.format(np.mean(speed_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([\"--config\", \"/projectnb/ece601/cliao25/SiamMask/experiments/siammask_sharp/config_davis.json\",\n",
    "                          \"--resume\", \"/projectnb/ece601/cliao25/SiamMask/experiments/siammask_sharp/snapshot/checkpoint_e10.pth\",\n",
    "                          \"--mask\", \"--refine\", \"--dataset\", \"DAVIS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a70aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69ca9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:18:11,255-rk0-features.py# 66] Current training 0 layers:\n",
      "\t\n",
      "[2021-04-26 23:18:11,257-rk0-features.py# 66] Current training 1 layers:\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "from custom import Custom\n",
    "model = Custom(anchors=cfg['anchors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d1e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:18:11,443-rk0-load_helper.py# 31] load pretrained model from /projectnb/ece601/cliao25/SiamMask/experiments/siammask_sharp/snapshot/checkpoint_e10.pth\n",
      "[2021-04-26 23:18:11,532-rk0-load_helper.py# 25] remove prefix 'module.'\n",
      "[2021-04-26 23:18:11,534-rk0-load_helper.py# 18] used keys:356\n"
     ]
    }
   ],
   "source": [
    "assert isfile(args.resume), '{} is not a valid file'.format(args.resume)\n",
    "model = load_pretrain(model, args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51ab858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and not args.cpu) else 'cpu')\n",
    "model = model.to(device)\n",
    "# setup dataset\n",
    "dataset = load_dataset(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f1f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vos_enable = True\n",
    "total_lost = 0  # VOT\n",
    "iou_lists = []  # VOS\n",
    "speed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89185c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:18:14,679-rk0-<ipython-input-17-d17cd02ca5bf>#  4] Namespace(arch='Custom', config='/projectnb/ece601/cliao25/SiamMask/experiments/siammask_sharp/config_davis.json', cpu=False, dataset='DAVIS', debug=False, gt=False, log='log_test.txt', mask=True, refine=True, resume='/projectnb/ece601/cliao25/SiamMask/experiments/siammask_sharp/snapshot/checkpoint_e10.pth', save_mask=False, video='', visualization=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['bike-packing', 'blackswan', 'bmx-trees', 'breakdance', 'camel', 'car-roundabout', 'car-shadow', 'cows', 'dance-twirl', 'dog', 'dogs-jump', 'drift-chicane', 'drift-straight', 'goat', 'gold-fish', 'horsejump-high', 'india', 'judo', 'kite-surf', 'lab-coat', 'libby', 'loading', 'mbike-trick', 'motocross-jump', 'paragliding-launch', 'parkour', 'pigs', 'scooter-black', 'shooting', 'soapbox'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())\n",
    "init_log('global', logging.INFO)\n",
    "logger = logging.getLogger('global')\n",
    "logger.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa942b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:42:48,435-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbike-packing_1       IOU at 0.30: 0.5755\n",
      "[2021-04-26 23:42:48,436-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbike-packing_1       IOU at 0.35: 0.5555\n",
      "[2021-04-26 23:42:48,437-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbike-packing_1       IOU at 0.40: 0.5316\n",
      "[2021-04-26 23:42:48,437-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbike-packing_1       IOU at 0.45: 0.5040\n",
      "[2021-04-26 23:42:50,570-rk0-<ipython-input-38-e2f95e10377e>#550] (1) Video: bike-packing Time: 1.8s Speed: 38.3fps\n",
      "[2021-04-26 23:42:52,474-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectblackswan_1          IOU at 0.30: 0.8181\n",
      "[2021-04-26 23:42:52,475-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectblackswan_1          IOU at 0.35: 0.8303\n",
      "[2021-04-26 23:42:52,475-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectblackswan_1          IOU at 0.40: 0.8391\n",
      "[2021-04-26 23:42:52,475-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectblackswan_1          IOU at 0.45: 0.8449\n",
      "[2021-04-26 23:42:53,856-rk0-<ipython-input-38-e2f95e10377e>#550] (2) Video: blackswan    Time: 1.0s Speed: 47.1fps\n",
      "[2021-04-26 23:42:56,333-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbmx-trees_1          IOU at 0.30: 0.4999\n",
      "[2021-04-26 23:42:56,333-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbmx-trees_1          IOU at 0.35: 0.5067\n",
      "[2021-04-26 23:42:56,334-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbmx-trees_1          IOU at 0.40: 0.5109\n",
      "[2021-04-26 23:42:56,334-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbmx-trees_1          IOU at 0.45: 0.5118\n",
      "[2021-04-26 23:42:58,452-rk0-<ipython-input-38-e2f95e10377e>#550] (3) Video: bmx-trees    Time: 1.1s Speed: 71.3fps\n",
      "[2021-04-26 23:43:01,333-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbreakdance_1         IOU at 0.30: 0.3722\n",
      "[2021-04-26 23:43:01,335-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbreakdance_1         IOU at 0.35: 0.3632\n",
      "[2021-04-26 23:43:01,336-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbreakdance_1         IOU at 0.40: 0.3515\n",
      "[2021-04-26 23:43:01,336-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectbreakdance_1         IOU at 0.45: 0.3384\n",
      "[2021-04-26 23:43:03,386-rk0-<ipython-input-38-e2f95e10377e>#550] (4) Video: breakdance   Time: 1.3s Speed: 62.4fps\n",
      "[2021-04-26 23:43:06,952-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcamel_1              IOU at 0.30: 0.6382\n",
      "[2021-04-26 23:43:06,953-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcamel_1              IOU at 0.35: 0.6492\n",
      "[2021-04-26 23:43:06,954-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcamel_1              IOU at 0.40: 0.6592\n",
      "[2021-04-26 23:43:06,954-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcamel_1              IOU at 0.45: 0.6686\n",
      "[2021-04-26 23:43:09,618-rk0-<ipython-input-38-e2f95e10377e>#550] (5) Video: camel        Time: 1.9s Speed: 45.8fps\n",
      "[2021-04-26 23:43:12,342-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-roundabout_1     IOU at 0.30: 0.9105\n",
      "[2021-04-26 23:43:12,343-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-roundabout_1     IOU at 0.35: 0.9141\n",
      "[2021-04-26 23:43:12,343-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-roundabout_1     IOU at 0.40: 0.9153\n",
      "[2021-04-26 23:43:12,343-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-roundabout_1     IOU at 0.45: 0.9144\n",
      "[2021-04-26 23:43:14,319-rk0-<ipython-input-38-e2f95e10377e>#550] (6) Video: car-roundabout Time: 1.4s Speed: 52.0fps\n",
      "[2021-04-26 23:43:15,604-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-shadow_1         IOU at 0.30: 0.9161\n",
      "[2021-04-26 23:43:15,605-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-shadow_1         IOU at 0.35: 0.9222\n",
      "[2021-04-26 23:43:15,605-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-shadow_1         IOU at 0.40: 0.9265\n",
      "[2021-04-26 23:43:15,605-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcar-shadow_1         IOU at 0.45: 0.9292\n",
      "[2021-04-26 23:43:16,706-rk0-<ipython-input-38-e2f95e10377e>#550] (7) Video: car-shadow   Time: 0.6s Speed: 66.5fps\n",
      "[2021-04-26 23:43:20,539-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcows_1               IOU at 0.30: 0.8128\n",
      "[2021-04-26 23:43:20,540-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcows_1               IOU at 0.35: 0.8197\n",
      "[2021-04-26 23:43:20,540-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcows_1               IOU at 0.40: 0.8238\n",
      "[2021-04-26 23:43:20,541-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectcows_1               IOU at 0.45: 0.8255\n",
      "[2021-04-26 23:43:23,603-rk0-<ipython-input-38-e2f95e10377e>#550] (8) Video: cows         Time: 1.9s Speed: 53.7fps\n",
      "[2021-04-26 23:43:26,748-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdance-twirl_1        IOU at 0.30: 0.6716\n",
      "[2021-04-26 23:43:26,749-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdance-twirl_1        IOU at 0.35: 0.6668\n",
      "[2021-04-26 23:43:26,749-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdance-twirl_1        IOU at 0.40: 0.6590\n",
      "[2021-04-26 23:43:26,750-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdance-twirl_1        IOU at 0.45: 0.6481\n",
      "[2021-04-26 23:43:29,137-rk0-<ipython-input-38-e2f95e10377e>#550] (9) Video: dance-twirl  Time: 1.6s Speed: 57.1fps\n",
      "[2021-04-26 23:43:31,257-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdog_1                IOU at 0.30: 0.7594\n",
      "[2021-04-26 23:43:31,258-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdog_1                IOU at 0.35: 0.7625\n",
      "[2021-04-26 23:43:31,258-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdog_1                IOU at 0.40: 0.7641\n",
      "[2021-04-26 23:43:31,259-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdog_1                IOU at 0.45: 0.7645\n",
      "[2021-04-26 23:43:32,984-rk0-<ipython-input-38-e2f95e10377e>#550] (10) Video: dog          Time: 1.1s Speed: 55.0fps\n",
      "[2021-04-26 23:43:35,136-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdogs-jump_1          IOU at 0.30: 0.3844\n",
      "[2021-04-26 23:43:35,137-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdogs-jump_1          IOU at 0.35: 0.3782\n",
      "[2021-04-26 23:43:35,137-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdogs-jump_1          IOU at 0.40: 0.3713\n",
      "[2021-04-26 23:43:35,138-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdogs-jump_1          IOU at 0.45: 0.3634\n",
      "[2021-04-26 23:43:36,823-rk0-<ipython-input-38-e2f95e10377e>#550] (11) Video: dogs-jump    Time: 1.0s Speed: 65.9fps\n",
      "[2021-04-26 23:43:38,396-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-chicane_1      IOU at 0.30: 0.7943\n",
      "[2021-04-26 23:43:38,397-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-chicane_1      IOU at 0.35: 0.7850\n",
      "[2021-04-26 23:43:38,398-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-chicane_1      IOU at 0.40: 0.7726\n",
      "[2021-04-26 23:43:38,398-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-chicane_1      IOU at 0.45: 0.7582\n",
      "[2021-04-26 23:43:39,678-rk0-<ipython-input-38-e2f95e10377e>#550] (12) Video: drift-chicane Time: 0.7s Speed: 74.5fps\n",
      "[2021-04-26 23:43:41,466-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-straight_1     IOU at 0.30: 0.8578\n",
      "[2021-04-26 23:43:41,467-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-straight_1     IOU at 0.35: 0.8614\n",
      "[2021-04-26 23:43:41,468-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-straight_1     IOU at 0.40: 0.8635\n",
      "[2021-04-26 23:43:41,468-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectdrift-straight_1     IOU at 0.45: 0.8639\n",
      "[2021-04-26 23:43:42,770-rk0-<ipython-input-38-e2f95e10377e>#550] (13) Video: drift-straight Time: 0.9s Speed: 55.3fps\n",
      "[2021-04-26 23:43:45,865-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgoat_1               IOU at 0.30: 0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:43:45,866-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgoat_1               IOU at 0.35: 0.7929\n",
      "[2021-04-26 23:43:45,867-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgoat_1               IOU at 0.40: 0.7967\n",
      "[2021-04-26 23:43:45,867-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgoat_1               IOU at 0.45: 0.7983\n",
      "[2021-04-26 23:43:48,356-rk0-<ipython-input-38-e2f95e10377e>#550] (14) Video: goat         Time: 1.5s Speed: 60.7fps\n",
      "[2021-04-26 23:43:52,211-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgold-fish_1          IOU at 0.30: 0.3502\n",
      "[2021-04-26 23:43:52,212-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgold-fish_1          IOU at 0.35: 0.3529\n",
      "[2021-04-26 23:43:52,213-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgold-fish_1          IOU at 0.40: 0.3554\n",
      "[2021-04-26 23:43:52,213-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectgold-fish_1          IOU at 0.45: 0.3547\n",
      "[2021-04-26 23:43:54,397-rk0-<ipython-input-38-e2f95e10377e>#550] (15) Video: gold-fish    Time: 2.5s Speed: 31.0fps\n",
      "[2021-04-26 23:43:56,098-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objecthorsejump-high_1     IOU at 0.30: 0.6788\n",
      "[2021-04-26 23:43:56,099-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objecthorsejump-high_1     IOU at 0.35: 0.6754\n",
      "[2021-04-26 23:43:56,099-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objecthorsejump-high_1     IOU at 0.40: 0.6693\n",
      "[2021-04-26 23:43:56,099-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objecthorsejump-high_1     IOU at 0.45: 0.6586\n",
      "[2021-04-26 23:43:57,421-rk0-<ipython-input-38-e2f95e10377e>#550] (16) Video: horsejump-high Time: 0.8s Speed: 59.2fps\n",
      "[2021-04-26 23:44:01,049-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectindia_1              IOU at 0.30: 0.3265\n",
      "[2021-04-26 23:44:01,050-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectindia_1              IOU at 0.35: 0.3095\n",
      "[2021-04-26 23:44:01,050-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectindia_1              IOU at 0.40: 0.2883\n",
      "[2021-04-26 23:44:01,050-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectindia_1              IOU at 0.45: 0.2636\n",
      "[2021-04-26 23:44:03,144-rk0-<ipython-input-38-e2f95e10377e>#550] (17) Video: india        Time: 2.2s Speed: 35.9fps\n",
      "[2021-04-26 23:44:04,294-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectjudo_1               IOU at 0.30: 0.6595\n",
      "[2021-04-26 23:44:04,295-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectjudo_1               IOU at 0.35: 0.6642\n",
      "[2021-04-26 23:44:04,296-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectjudo_1               IOU at 0.40: 0.6666\n",
      "[2021-04-26 23:44:04,296-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectjudo_1               IOU at 0.45: 0.6668\n",
      "[2021-04-26 23:44:05,136-rk0-<ipython-input-38-e2f95e10377e>#550] (18) Video: judo         Time: 0.6s Speed: 57.2fps\n",
      "[2021-04-26 23:44:06,803-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectkite-surf_1          IOU at 0.30: 0.3139\n",
      "[2021-04-26 23:44:06,804-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectkite-surf_1          IOU at 0.35: 0.2963\n",
      "[2021-04-26 23:44:06,805-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectkite-surf_1          IOU at 0.40: 0.2805\n",
      "[2021-04-26 23:44:06,805-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectkite-surf_1          IOU at 0.45: 0.2619\n",
      "[2021-04-26 23:44:08,161-rk0-<ipython-input-38-e2f95e10377e>#550] (19) Video: kite-surf    Time: 0.8s Speed: 63.0fps\n",
      "[2021-04-26 23:44:10,107-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlab-coat_1           IOU at 0.30: 0.8217\n",
      "[2021-04-26 23:44:10,108-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlab-coat_1           IOU at 0.35: 0.8172\n",
      "[2021-04-26 23:44:10,108-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlab-coat_1           IOU at 0.40: 0.8129\n",
      "[2021-04-26 23:44:10,108-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlab-coat_1           IOU at 0.45: 0.8083\n",
      "[2021-04-26 23:44:11,391-rk0-<ipython-input-38-e2f95e10377e>#550] (20) Video: lab-coat     Time: 1.1s Speed: 40.5fps\n",
      "[2021-04-26 23:44:12,973-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlibby_1              IOU at 0.30: 0.5468\n",
      "[2021-04-26 23:44:12,974-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlibby_1              IOU at 0.35: 0.5580\n",
      "[2021-04-26 23:44:12,975-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlibby_1              IOU at 0.40: 0.5659\n",
      "[2021-04-26 23:44:12,975-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectlibby_1              IOU at 0.45: 0.5690\n",
      "[2021-04-26 23:44:14,325-rk0-<ipython-input-38-e2f95e10377e>#550] (21) Video: libby        Time: 0.8s Speed: 63.9fps\n",
      "[2021-04-26 23:44:16,522-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectloading_1            IOU at 0.30: 0.3716\n",
      "[2021-04-26 23:44:16,523-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectloading_1            IOU at 0.35: 0.3287\n",
      "[2021-04-26 23:44:16,524-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectloading_1            IOU at 0.40: 0.2848\n",
      "[2021-04-26 23:44:16,524-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectloading_1            IOU at 0.45: 0.2431\n",
      "[2021-04-26 23:44:17,942-rk0-<ipython-input-38-e2f95e10377e>#550] (22) Video: loading      Time: 1.3s Speed: 37.5fps\n",
      "[2021-04-26 23:44:20,424-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmbike-trick_1        IOU at 0.30: 0.5836\n",
      "[2021-04-26 23:44:20,425-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmbike-trick_1        IOU at 0.35: 0.5632\n",
      "[2021-04-26 23:44:20,425-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmbike-trick_1        IOU at 0.40: 0.5420\n",
      "[2021-04-26 23:44:20,426-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmbike-trick_1        IOU at 0.45: 0.5200\n",
      "[2021-04-26 23:44:22,333-rk0-<ipython-input-38-e2f95e10377e>#550] (23) Video: mbike-trick  Time: 1.1s Speed: 68.8fps\n",
      "[2021-04-26 23:44:23,818-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmotocross-jump_1     IOU at 0.30: 0.6560\n",
      "[2021-04-26 23:44:23,819-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmotocross-jump_1     IOU at 0.35: 0.6349\n",
      "[2021-04-26 23:44:23,820-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmotocross-jump_1     IOU at 0.40: 0.6124\n",
      "[2021-04-26 23:44:23,820-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectmotocross-jump_1     IOU at 0.45: 0.5872\n",
      "[2021-04-26 23:44:24,895-rk0-<ipython-input-38-e2f95e10377e>#550] (24) Video: motocross-jump Time: 0.8s Speed: 48.0fps\n",
      "[2021-04-26 23:44:27,521-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparagliding-launch_1 IOU at 0.30: 0.5748\n",
      "[2021-04-26 23:44:27,522-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparagliding-launch_1 IOU at 0.35: 0.5717\n",
      "[2021-04-26 23:44:27,522-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparagliding-launch_1 IOU at 0.40: 0.5646\n",
      "[2021-04-26 23:44:27,523-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparagliding-launch_1 IOU at 0.45: 0.5544\n",
      "[2021-04-26 23:44:29,774-rk0-<ipython-input-38-e2f95e10377e>#550] (25) Video: paragliding-launch Time: 1.2s Speed: 64.9fps\n",
      "[2021-04-26 23:44:32,982-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparkour_1            IOU at 0.30: 0.8041\n",
      "[2021-04-26 23:44:32,983-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparkour_1            IOU at 0.35: 0.8096\n",
      "[2021-04-26 23:44:32,984-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparkour_1            IOU at 0.40: 0.8125\n",
      "[2021-04-26 23:44:32,984-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectparkour_1            IOU at 0.45: 0.8128\n",
      "[2021-04-26 23:44:35,751-rk0-<ipython-input-38-e2f95e10377e>#550] (26) Video: parkour      Time: 1.4s Speed: 69.1fps\n",
      "[2021-04-26 23:44:39,685-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectpigs_1               IOU at 0.30: 0.7247\n",
      "[2021-04-26 23:44:39,686-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectpigs_1               IOU at 0.35: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-26 23:44:39,686-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectpigs_1               IOU at 0.40: 0.6938\n",
      "[2021-04-26 23:44:39,686-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectpigs_1               IOU at 0.45: 0.6758\n",
      "[2021-04-26 23:44:42,078-rk0-<ipython-input-38-e2f95e10377e>#550] (27) Video: pigs         Time: 2.5s Speed: 31.7fps\n",
      "[2021-04-26 23:44:43,645-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectscooter-black_1      IOU at 0.30: 0.7733\n",
      "[2021-04-26 23:44:43,646-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectscooter-black_1      IOU at 0.35: 0.7694\n",
      "[2021-04-26 23:44:43,646-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectscooter-black_1      IOU at 0.40: 0.7605\n",
      "[2021-04-26 23:44:43,647-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectscooter-black_1      IOU at 0.45: 0.7486\n",
      "[2021-04-26 23:44:44,924-rk0-<ipython-input-38-e2f95e10377e>#550] (28) Video: scooter-black Time: 0.8s Speed: 53.9fps\n",
      "[2021-04-26 23:44:46,908-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectshooting_1           IOU at 0.30: 0.7387\n",
      "[2021-04-26 23:44:46,909-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectshooting_1           IOU at 0.35: 0.7335\n",
      "[2021-04-26 23:44:46,909-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectshooting_1           IOU at 0.40: 0.7272\n",
      "[2021-04-26 23:44:46,910-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectshooting_1           IOU at 0.45: 0.7183\n",
      "[2021-04-26 23:44:48,256-rk0-<ipython-input-38-e2f95e10377e>#550] (29) Video: shooting     Time: 1.0s Speed: 37.3fps\n",
      "[2021-04-26 23:44:51,768-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectsoapbox_1            IOU at 0.30: 0.5744\n",
      "[2021-04-26 23:44:51,769-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectsoapbox_1            IOU at 0.35: 0.5566\n",
      "[2021-04-26 23:44:51,769-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectsoapbox_1            IOU at 0.40: 0.5345\n",
      "[2021-04-26 23:44:51,769-rk0-<ipython-input-38-e2f95e10377e>#517] Fusion Multi Objectsoapbox_1            IOU at 0.45: 0.5082\n",
      "[2021-04-26 23:44:54,409-rk0-<ipython-input-38-e2f95e10377e>#550] (30) Video: soapbox      Time: 1.7s Speed: 56.9fps\n"
     ]
    }
   ],
   "source": [
    "v_id = 0\n",
    "video = 'loading'\n",
    "\n",
    "for v_id, video in enumerate(dataset.keys(), start=1):\n",
    "    if vos_enable:\n",
    "        iou_list, speed = track_vos(model, dataset[video], video, cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                             args.mask, args.refine, args.dataset in ['DAVIS2017', 'ytb_vos'], device=device)\n",
    "        iou_lists.append(iou_list)\n",
    "    else:\n",
    "        lost, speed = track_vot(model, dataset[video], cfg['hp'] if 'hp' in cfg.keys() else None,\n",
    "                         args.mask, args.refine, device=device)\n",
    "        total_lost += lost\n",
    "    speed_list.append(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecb2a624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike-packing'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HH.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(-1,-10,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d0add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
